{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n"
      ],
      "metadata": {
        "id": "EeGouJmHCDwk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing libaries and setting up setups"
      ],
      "metadata": {
        "id": "6CccCRHwCQCV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnXVKL5HBW-F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e2c6ea4-b250-4bb7-f5ec-4730a33d2f1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ptflops\n",
            "  Downloading ptflops-0.7.4-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from ptflops) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0->ptflops)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0->ptflops)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0->ptflops)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0->ptflops)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0->ptflops)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0->ptflops)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0->ptflops)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0->ptflops)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0->ptflops)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0->ptflops)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->ptflops) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->ptflops) (3.0.2)\n",
            "Downloading ptflops-0.7.4-py3-none-any.whl (19 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ptflops\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ptflops-0.7.4\n",
            "Using device: cpu\n",
            "WARNING: Training will be very slow without GPU!\n"
          ]
        }
      ],
      "source": [
        "# Importing and setups\n",
        "!pip install ptflops # need to install everytime either cpu or gpu\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, classification_report, roc_curve, auc\n",
        "import csv\n",
        "from datetime import datetime\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "from ptflops import get_model_complexity_info\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)  # also need to set cuda seed\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True  # reproducible\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if device.type == 'cpu':\n",
        "    print(\"WARNING: Training will be very slow without GPU!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing dataset"
      ],
      "metadata": {
        "id": "oduxXn4yNGId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preparation with Augmentation\n",
        "class CIFAR10DataModule:\n",
        "    def __init__(self, batch_size=128, num_workers=4):\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "\n",
        "        # CIFAR10 normalization values - DON'T CHANGE\n",
        "        self.mean = (0.4914, 0.4822, 0.4465)\n",
        "        self.std = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "        # Define transformations\n",
        "        self.train_transform = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),  # standard augmentation\n",
        "            transforms.RandAugment(num_ops=2, magnitude=9),  # tried 3 ops but too aggressive\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(self.mean, self.std)\n",
        "        ])\n",
        "\n",
        "        # No augmentation for test set\n",
        "        self.test_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(self.mean, self.std)\n",
        "        ])\n",
        "\n",
        "    def setup(self):\n",
        "        # Download datasets\n",
        "        print(\"Setting up datasets...\")\n",
        "        self.train_dataset = datasets.CIFAR10(\n",
        "            root='./data',\n",
        "            train=True,\n",
        "            download=True,\n",
        "            transform=self.train_transform\n",
        "        )\n",
        "\n",
        "        self.val_dataset = datasets.CIFAR10(\n",
        "            root='./data',\n",
        "            train=False,\n",
        "            download=True,\n",
        "            transform=self.test_transform\n",
        "        )\n",
        "        print(f\"Loaded {len(self.train_dataset)} training and {len(self.val_dataset)} validation samples\")\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,  # important for training!\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=True  # helps if using GPU\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,  # no need to shuffle for validation\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=True\n",
        "        )"
      ],
      "metadata": {
        "id": "6W2cIztxNKQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "patch embedding"
      ],
      "metadata": {
        "id": "un6f2A1k_Y0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Patch Embedding Layer\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=192):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "        # Originally used a linear layer here, but conv is more efficient and does the same thing\n",
        "        self.proj = nn.Conv2d(\n",
        "            in_channels,\n",
        "            embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=patch_size\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [B, C, H, W]\n",
        "        B, C, H, W = x.shape\n",
        "        assert H == self.img_size and W == self.img_size, \\\n",
        "            f\"Input image size ({H}*{W}) doesn't match expected size ({self.img_size}*{self.img_size})\"\n",
        "\n",
        "        # [B, C, H, W] -> [B, E, H/P, W/P] -> [B, E, (H/P)*(W/P)] -> [B, (H/P)*(W/P), E]\n",
        "        x = self.proj(x)  # [B, E, H/P, W/P]\n",
        "        x = x.flatten(2)  # [B, E, (H/P)*(W/P)]\n",
        "        x = x.transpose(1, 2)  # [B, (H/P)*(W/P), E]\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "-vO3SW-aNPe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi Head Attention (MHA)"
      ],
      "metadata": {
        "id": "wXoOpG3Jj7FP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Head Self-Attention\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim=192, num_heads=8, dropout=0.1): # 192/8 = 24 per head\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        # Double-check dimensions\n",
        "        assert self.head_dim * num_heads == embed_dim, \\\n",
        "            f\"embed_dim {embed_dim} must be divisible by num_heads {num_heads}\"\n",
        "\n",
        "        # Combined QKV projections\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "        self.proj_dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [B, N, E] - B=batch, N=sequence_length, E=embedding_dim\n",
        "        B, N, E = x.shape\n",
        "\n",
        "        # Project to Q, K, V and reshape for multi-head attention\n",
        "        # This is that fancy reshape for multi-head attention\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]  # [B, H, N, D] - H=heads, D=head_dim\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        # The scaling is super important - training dies without it\n",
        "        attn = (q @ k.transpose(-2, -1)) * (1.0 / np.sqrt(self.head_dim))  # [B, H, N, N]\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        attn = self.attn_dropout(attn)  # helps generalization\n",
        "\n",
        "        # Apply attention to values\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, E)  # [B, N, E]\n",
        "        x = self.proj(x)  # final projection\n",
        "        x = self.proj_dropout(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "gOMZzd3ZkDFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP"
      ],
      "metadata": {
        "id": "ZsTpqPnTkHDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP Block\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features, out_features, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        # GELU Better than ReLU for transformers\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)  # second dropout seems to help\n",
        "        return x"
      ],
      "metadata": {
        "id": "ZCtKwXAIkO-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer Encoder Block"
      ],
      "metadata": {
        "id": "AfChh78hk3nV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer Encoder Block\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim=192, num_heads=8, mlp_ratio=4.0, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads, dropout)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = MLP(\n",
        "            in_features=embed_dim,\n",
        "            hidden_features=int(embed_dim * mlp_ratio),  # the ratio matters!\n",
        "            out_features=embed_dim,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        # NOTE: we're using pre-norm formulation\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pre-norm formulation - more stable, can train deeper networks\n",
        "        # x + sublayer(norm(x)) instead of norm(x + sublayer(x))\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "5lrCTQ9lVXA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete Vision Transformer Model"
      ],
      "metadata": {
        "id": "C-o8M1ciV6Jp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete Vision Transformer Model\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size=32,\n",
        "        patch_size=4,  # 4x4 patches for CIFAR ie(32^2//4^2 == 64 tokens)\n",
        "        in_channels=3, # RGB channel\n",
        "        num_classes=10,# number of expected outputs\n",
        "        embed_dim=192,  # tried 384 but too many params for CIFAR tend to overfit\n",
        "        depth=9,  # paper uses 12, but 9 is enough for CIFAR and 12 tend to overfit\n",
        "        num_heads=8,  # must divide embed_dim evenly 192/8 = 24\n",
        "        mlp_ratio=4.0,\n",
        "        dropout=0.1, # probablity of skiping connection ie 10 percent\n",
        "        embed_dropout=0.1  # separate dropout rate for embeddings\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_tokens = (img_size // patch_size) ** 2\n",
        "\n",
        "        # Patch embedding\n",
        "        self.patch_embed = PatchEmbedding(\n",
        "            img_size=img_size,\n",
        "            patch_size=patch_size,\n",
        "            in_channels=in_channels,\n",
        "            embed_dim=embed_dim\n",
        "        )\n",
        "\n",
        "        # Class token and position embeddings\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        # Position embeddings - could use sinusoidal but learned works fine\n",
        "        # postional embeddings are used because we have 8 multi head attention we need assign position for each vector\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_tokens + 1, embed_dim))\n",
        "\n",
        "        # Initialize weights for faster convergence\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "\n",
        "        self.dropout = nn.Dropout(embed_dropout)\n",
        "\n",
        "        # Transformer blocks - this is the main part of the model\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(\n",
        "                embed_dim=embed_dim,\n",
        "                num_heads=num_heads,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                dropout=dropout\n",
        "            )\n",
        "            for _ in range(depth) # we just use for loop instead rewriting tranformer 8 times\n",
        "        ])\n",
        "\n",
        "        # Final normalization layer\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # Classification head - just a linear layer\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        # How many params?\n",
        "        #print(f\"ViT params: {sum(p.numel() for p in self.parameters())}\")\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        # Weight initialization matters for transformers!\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [B, C, H, W]\n",
        "        B = x.shape[0]\n",
        "\n",
        "        # Create patch embeddings\n",
        "        x = self.patch_embed(x)  # [B, N, E]\n",
        "\n",
        "        # Add class token - used for final classification\n",
        "        cls_token = self.cls_token.expand(B, -1, -1)  # [B, 1, E]\n",
        "        x = torch.cat((cls_token, x), dim=1)  # [B, N+1, E]\n",
        "\n",
        "        # Add position embeddings and apply dropout\n",
        "        x = x + self.pos_embed  # broadcasting takes care of batch dim\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            # Could add intermediate supervision here?\n",
        "            # Tried it, didn't help much, so removed it\n",
        "            x = block(x)\n",
        "\n",
        "        # Apply final normalization\n",
        "        x = self.norm(x)\n",
        "\n",
        "        # Take class token for classification\n",
        "        # Could use pooling over all tokens but this works better\n",
        "        x = x[:, 0]  # just get CLS token\n",
        "\n",
        "        # Classification head\n",
        "        x = self.head(x)\n",
        "        # Could add an extra non-linearity here but linear seems fine\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "lJu_WupdV7Dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "training and eval"
      ],
      "metadata": {
        "id": "ABuApCfWNWTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and Evaluation Utilities\n",
        "def train_one_epoch(model, train_loader, criterion, optimizer, scheduler, device):\n",
        "    model.train()  # set model to training mode\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    batch_time = 0.0\n",
        "\n",
        "    # Progress bar\n",
        "    pbar = tqdm(train_loader, desc=\"Training\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        batch_start = time.time()\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()  # clear gradients first\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Could add gradient clipping here\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        # But Adam seems to work fine without it\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update learning rate - using per-step scheduler\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # Track metrics\n",
        "        total_loss += loss.item() * data.size(0)\n",
        "        _, predicted = output.max(1)  # get predicted class\n",
        "        total += target.size(0)\n",
        "        correct += predicted.eq(target).sum().item()\n",
        "\n",
        "        # Track batch time\n",
        "        batch_end = time.time()\n",
        "        batch_time += (batch_end - batch_start)\n",
        "\n",
        "        # Update progress bar - helps to see how training is going\n",
        "        pbar.set_postfix({\n",
        "            \"loss\": f\"{loss.item():.4f}\",\n",
        "            \"acc\": f\"{100. * correct / total:.1f}%\",\n",
        "            #\"lr\": f\"{optimizer.param_groups[0]['lr']:.6f}\"  # uncomment for debugging\n",
        "        })\n",
        "\n",
        "\n",
        "    epoch_time = time.time() - start_time\n",
        "\n",
        "    return total_loss / len(train_loader.dataset), 100. * correct / total, epoch_time, batch_time / len(train_loader)\n",
        "\n",
        "\n",
        "def evaluate(model, val_loader, criterion, device, classes=None, full_metrics=False):\n",
        "    model.eval()  # set model to evaluation mode\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    inference_times = []\n",
        "\n",
        "    # For confusion matrix and per-class metrics\n",
        "    all_targets = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():  # no need to track gradients during evaluation\n",
        "        for data, target in tqdm(val_loader, desc=\"Evaluation\"):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Measure inference time\n",
        "            start_time = time.time()\n",
        "            output = model(data)\n",
        "            inference_time = time.time() - start_time\n",
        "            inference_times.append(inference_time)\n",
        "\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            # Track metrics\n",
        "            total_loss += loss.item() * data.size(0)\n",
        "            _, predicted = output.max(1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "\n",
        "            # Store targets and predictions for additional metrics\n",
        "            all_targets.extend(target.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    # Compute aggregate metrics\n",
        "    avg_loss = total_loss / len(val_loader.dataset)\n",
        "    accuracy = 100. * correct / total\n",
        "    avg_inference_time = sum(inference_times) / len(inference_times)\n",
        "\n",
        "    results = {\n",
        "        'loss': avg_loss,\n",
        "        'accuracy': accuracy,\n",
        "        'inference_time_ms': avg_inference_time * 1000  # Convert to ms\n",
        "    }\n",
        "\n",
        "    # Add detailed metrics if requested\n",
        "    if full_metrics and classes:\n",
        "        # Calculate per-class precision, recall, f1-score\n",
        "        # Can't skip this computation - might seem slow but it's useful info\n",
        "        precision, recall, f1, support = precision_recall_fscore_support(\n",
        "            all_targets, all_predictions, labels=range(len(classes)), average=None\n",
        "        )\n",
        "\n",
        "        # Create confusion matrix\n",
        "        cm = confusion_matrix(all_targets, all_predictions, labels=range(len(classes)))\n",
        "\n",
        "        # Add to results\n",
        "        results['confusion_matrix'] = cm\n",
        "        results['per_class'] = {\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'support': support\n",
        "        }\n",
        "        results['classes'] = classes\n",
        "        results['targets'] = all_targets\n",
        "        results['predictions'] = all_predictions\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "Q1VeeRkkNKOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "metrics and visualization function"
      ],
      "metadata": {
        "id": "gTx_pwaSN1Eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics and Visualization Functions\n",
        "def calculate_and_plot_metrics(model, val_loader, criterion, device, classes):\n",
        "    print(\"Calculating detailed metrics...\")\n",
        "    results = evaluate(model, val_loader, criterion, device, classes, full_metrics=True)\n",
        "\n",
        "    # results\n",
        "    cm = results['confusion_matrix']\n",
        "    per_class = results['per_class']\n",
        "    targets = results['targets']\n",
        "    predictions = results['predictions']\n",
        "\n",
        "    # 1. Plot confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    # Tried various colormaps - Blues is most readable\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('vit_confusion_matrix.png', dpi=200)  # higher DPI for paper-quality\n",
        "\n",
        "    # 2. Plot per-class metrics\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    x = np.arange(len(classes))\n",
        "    width = 0.2  # width of bars\n",
        "\n",
        "    # Plot bar chart with precision, recall, F1\n",
        "    plt.bar(x - width, per_class['precision'], width, label='Precision')\n",
        "    plt.bar(x, per_class['recall'], width, label='Recall')\n",
        "    plt.bar(x + width, per_class['f1'], width, label='F1-Score')\n",
        "\n",
        "    plt.xlabel('Classes')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Per-Class Performance Metrics')\n",
        "    plt.xticks(x, classes, rotation=45)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('vit_per_class_metrics.png')\n",
        "\n",
        "    # 3. Compute and plot ROC curves (one-vs-rest)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    # Prepare one-hot encoded targets for ROC\n",
        "    target_one_hot = np.zeros((len(targets), len(classes)))\n",
        "    for i, t in enumerate(targets):\n",
        "        target_one_hot[i, t] = 1\n",
        "\n",
        "    # Get probability outputs for all samples\n",
        "    # Need to rerun the model to get probabilities\n",
        "    all_probs = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data, _ in val_loader:\n",
        "            data = data.to(device)\n",
        "            outputs = model(data)\n",
        "            probs = F.softmax(outputs, dim=1).cpu().numpy()\n",
        "            all_probs.append(probs)\n",
        "\n",
        "    all_probs = np.vstack(all_probs)\n",
        "\n",
        "    # Plot ROC curve for each class\n",
        "    mean_auc = 0\n",
        "    for i, cls in enumerate(classes):\n",
        "        fpr, tpr, _ = roc_curve(target_one_hot[:, i], all_probs[:, i])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        mean_auc += roc_auc\n",
        "        plt.plot(fpr, tpr, lw=2, label=f'{cls} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "    mean_auc /= len(classes)\n",
        "\n",
        "    # Add diagonal line (random classifier)\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'ROC Curves (Mean AUC = {mean_auc:.2f})')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('vit_roc_curves.png')\n",
        "\n",
        "    # Return metrics for CSV export\n",
        "    return {\n",
        "        'accuracy': results['accuracy'],\n",
        "        'loss': results['loss'],\n",
        "        'inference_time_ms': results['inference_time_ms'],\n",
        "        'per_class_precision': per_class['precision'],\n",
        "        'per_class_recall': per_class['recall'],\n",
        "        'per_class_f1': per_class['f1'],\n",
        "        'mean_auc': mean_auc\n",
        "    }\n",
        "\n",
        "\n",
        "# Calculate model complexity\n",
        "def calculate_model_complexity(model, input_size=(3, 32, 32)):\n",
        "    print(\"Calculating model complexity...\")\n",
        "    macs, params = get_model_complexity_info(\n",
        "        model, input_size, as_strings=False, print_per_layer_stat=False\n",
        "    )\n",
        "\n",
        "    # Did you know? FLOPs ≈ 2 * MACs\n",
        "    # ptflops returns MACs, but papers usually report FLOPs\n",
        "    return {\n",
        "        'params': params,\n",
        "        'flops': macs * 2,  # Convert MACs to FLOPs\n",
        "        'params_millions': params / 1e6,\n",
        "        'flops_billions': macs * 2 / 1e9\n",
        "    }\n",
        "\n",
        "\n",
        "# Export metrics to CSV\n",
        "def export_metrics_to_csv(metrics, model_name='ViT', filename='model_metrics.csv'):\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs('metrics', exist_ok=True)\n",
        "\n",
        "    # Prepare CSV file path with timestamp\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    filepath = f'metrics/{model_name}_{timestamp}.csv'\n",
        "\n",
        "    # Flatten nested dictionaries\n",
        "    flat_metrics = {}\n",
        "    for key, value in metrics.items():\n",
        "        if isinstance(value, dict):\n",
        "            for subkey, subvalue in value.items():\n",
        "                flat_metrics[f'{key}_{subkey}'] = subvalue\n",
        "        elif isinstance(value, np.ndarray):\n",
        "            for i, val in enumerate(value):\n",
        "                flat_metrics[f'{key}_{i}'] = val\n",
        "        else:\n",
        "            flat_metrics[key] = value\n",
        "\n",
        "    # Write to CSV\n",
        "    with open(filepath, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "\n",
        "        # Write header\n",
        "        writer.writerow(['Metric', 'Value'])\n",
        "\n",
        "        # Write metrics\n",
        "        for key, value in flat_metrics.items():\n",
        "            writer.writerow([key, value])\n",
        "\n",
        "    print(f\"Metrics exported to {filepath}\")\n",
        "\n",
        "    # Also create a summary CSV for model comparison\n",
        "    # This is super handy when doing hyperparameter sweeps!\n",
        "    summary_path = 'metrics/model_comparison.csv'\n",
        "\n",
        "    # Check if summary file exists, create with header if not\n",
        "    file_exists = os.path.isfile(summary_path)\n",
        "    with open(summary_path, 'a', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "\n",
        "        if not file_exists:\n",
        "            writer.writerow([\n",
        "                'Model', 'Accuracy', 'Loss', 'Params (M)', 'FLOPs (G)',\n",
        "                'Inference Time (ms)', 'Mean AUC', 'Training Time (s)'\n",
        "            ])\n",
        "\n",
        "\n",
        "        writer.writerow([\n",
        "            model_name,\n",
        "            metrics['accuracy'],\n",
        "            metrics['loss'],\n",
        "            metrics['complexity']['params_millions'],\n",
        "            metrics['complexity']['flops_billions'],\n",
        "            metrics['inference_time_ms'],\n",
        "            metrics['mean_auc'],\n",
        "            metrics['training_time']\n",
        "        ])\n",
        "\n",
        "    print(f\"Summary metrics added to {summary_path}\")"
      ],
      "metadata": {
        "id": "wud0aHcjN1bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main training function"
      ],
      "metadata": {
        "id": "L8-jOwu9tQbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Training Function\n",
        "def train_vit_cifar10(epochs=100, batch_size=128, lr=1e-3, warmup_epochs=5, model_name='ViT'):\n",
        "    # Setup data\n",
        "    print(f\"\\n=== Setting up {model_name} training ===\")\n",
        "    print(f\"Epochs: {epochs}, Batch size: {batch_size}, LR: {lr}\")\n",
        "\n",
        "    data_module = CIFAR10DataModule(batch_size=batch_size)\n",
        "    data_module.setup()\n",
        "    train_loader = data_module.train_dataloader()\n",
        "    val_loader = data_module.val_dataloader()\n",
        "\n",
        "    # CIFAR-10 classes\n",
        "    classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "    # Create model - this is the standard ViT config for CIFAR\n",
        "    model = VisionTransformer(\n",
        "        img_size=32,\n",
        "        patch_size=4,  # 4x4 patches, so 8x8=64 patches total\n",
        "        in_channels=3,\n",
        "        num_classes=10,\n",
        "        embed_dim=192,  # tried 384 but it was overkill\n",
        "        depth=9,\n",
        "        num_heads=8,  # 192 / 8 = 24 dim per head\n",
        "        mlp_ratio=4.0,\n",
        "        dropout=0.1,  # dropout helps a lot on CIFAR\n",
        "        embed_dropout=0.1\n",
        "    ).to(device)\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "    # Calculate model complexity\n",
        "    complexity = calculate_model_complexity(model)\n",
        "    print(f\"FLOPs: {complexity['flops_billions']:.2f} G\")\n",
        "    print(f\"Parameters: {complexity['params_millions']:.2f} M\")\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Optimizer\n",
        "    #  AdamW works better for transformers\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.05)\n",
        "\n",
        "    # Learning rate scheduler - cosine decay with warmup\n",
        "    # Warmup is crucial for transformer training stability\n",
        "    total_steps = len(train_loader) * epochs\n",
        "    warmup_steps = len(train_loader) * warmup_epochs\n",
        "\n",
        "    # Learning rate schedule\n",
        "    def lr_lambda(step):\n",
        "        # Linear warmup + cosine decay\n",
        "        if step < warmup_steps:\n",
        "            return float(step) / float(max(1, warmup_steps))\n",
        "        # Cosine annealing\n",
        "        return 0.5 * (1.0 + np.cos(np.pi * float(step - warmup_steps) / float(total_steps - warmup_steps)))\n",
        "\n",
        "    # Create scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "    # Training loop\n",
        "    print(\"\\n=== Starting training ===\")\n",
        "    best_acc = 0.0\n",
        "    train_losses, train_accs = [], []\n",
        "    val_losses, val_accs = [], []\n",
        "    epoch_times, batch_times = [], []\n",
        "    total_training_time = 0\n",
        "    lr_history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        # Log learning rate\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        lr_history.append(current_lr)\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_acc, epoch_time, avg_batch_time = train_one_epoch(\n",
        "            model, train_loader, criterion, optimizer, scheduler, device\n",
        "        )\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        epoch_times.append(epoch_time)\n",
        "        batch_times.append(avg_batch_time)\n",
        "        total_training_time += epoch_time\n",
        "\n",
        "        # Evaluate\n",
        "        val_results = evaluate(model, val_loader, criterion, device)\n",
        "        val_loss = val_results['loss']\n",
        "        val_acc = val_results['accuracy']\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        # Print metrics\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "        print(f\"Epoch Time: {epoch_time:.2f}s, Avg Batch Time: {avg_batch_time*1000:.2f}ms\")\n",
        "        print(f\"Current LR: {current_lr:.6f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(model.state_dict(), \"vit_cifar10_best.pth\")\n",
        "            print(f\"New best validation accuracy: {best_acc:.2f}%!\")\n",
        "            # Also save at specific checkpoints (optional)\n",
        "            #if val_acc > 90:\n",
        "            #    torch.save(model.state_dict(), f\"vit_cifar10_{val_acc:.1f}.pth\")\n",
        "\n",
        "        # Early stopping check after a reasonable number of epochs\n",
        "        # No need to train forever if we're already good\n",
        "        if epoch >= 50 and best_acc >= 90.0:\n",
        "            print(f\"Reached target accuracy of 90%. Stopping early!\")\n",
        "            break\n",
        "\n",
        "    print(f\"\\n=== Training complete ===\")\n",
        "    print(f\"Total training time: {total_training_time:.2f}s\")\n",
        "    print(f\"Best validation accuracy: {best_acc:.2f}%\")\n",
        "\n",
        "    # Plot final expanded training metrics\n",
        "    print(\"\\nGenerating final training plots...\")\n",
        "    plt.figure(figsize=(18, 12))\n",
        "\n",
        "    # 1. Loss curves\n",
        "    plt.subplot(2, 3, 1)\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Val Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss Curves')\n",
        "    plt.legend()\n",
        "\n",
        "    # 2. Accuracy curves\n",
        "    plt.subplot(2, 3, 2)\n",
        "    plt.plot(train_accs, label='Train Accuracy')\n",
        "    plt.plot(val_accs, label='Val Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.title('Accuracy Curves')\n",
        "    plt.legend()\n",
        "\n",
        "    # 3. Epoch times\n",
        "    plt.subplot(2, 3, 3)\n",
        "    plt.plot(epoch_times)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Time (s)')\n",
        "    plt.title('Epoch Training Time')\n",
        "\n",
        "    # 4. Batch times\n",
        "    plt.subplot(2, 3, 4)\n",
        "    plt.plot(batch_times)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Time (s)')\n",
        "    plt.title('Average Batch Processing Time')\n",
        "\n",
        "    # 5. Learning rate\n",
        "    plt.subplot(2, 3, 5)\n",
        "    plt.plot(lr_history)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.title('Learning Rate Schedule')\n",
        "    # Add grid for readability\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('vit_training_metrics.png')\n",
        "    plt.close()  # close to avoid display issues with multiple plots\n",
        "\n",
        "    # Load best model for final evaluation\n",
        "    print(\"\\nLoading best model for final evaluation...\")\n",
        "    model.load_state_dict(torch.load(\"vit_cifar10_best.pth\"))\n",
        "\n",
        "    # Calculate detailed metrics\n",
        "    detailed_metrics = calculate_and_plot_metrics(model, val_loader, criterion, device, classes)\n",
        "\n",
        "    # Prepare metrics for export\n",
        "    final_metrics = {\n",
        "        'accuracy': best_acc,\n",
        "        'loss': val_losses[-1],\n",
        "        'inference_time_ms': detailed_metrics['inference_time_ms'],\n",
        "        'training_time': total_training_time,\n",
        "        'epochs': len(train_losses),\n",
        "        'avg_epoch_time': sum(epoch_times) / len(epoch_times),\n",
        "        'avg_batch_time': sum(batch_times) / len(batch_times),\n",
        "        'complexity': complexity,\n",
        "        'mean_auc': detailed_metrics['mean_auc'],\n",
        "        'per_class': {\n",
        "            'precision': detailed_metrics['per_class_precision'],\n",
        "            'recall': detailed_metrics['per_class_recall'],\n",
        "            'f1': detailed_metrics['per_class_f1']\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Export metrics to CSV\n",
        "    export_metrics_to_csv(final_metrics, model_name)\n",
        "\n",
        "    print(f\"Best validation accuracy: {best_acc:.2f}%\")\n",
        "    return model, best_acc, final_metrics"
      ],
      "metadata": {
        "id": "C4C2WFo8y3cE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention Visualization function"
      ],
      "metadata": {
        "id": "gHj2MGDmtXS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Attention Visualization Function\n",
        "def visualize_attention(model, dataloader, device, num_images=4):\n",
        "    # Get some test images\n",
        "    dataiter = iter(dataloader)\n",
        "    images, labels = next(dataiter)\n",
        "    images = images[:num_images].to(device)\n",
        "    labels = labels[:num_images]\n",
        "\n",
        "    # Get class names\n",
        "    classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "    # Set model to eval mode\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    def get_attention_maps(x):\n",
        "        B = x.shape[0]\n",
        "        x = model.patch_embed(x)\n",
        "        cls_token = model.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_token, x), dim=1)\n",
        "        x = x + model.pos_embed\n",
        "        x = model.dropout(x)\n",
        "\n",
        "        # Pass through transformer blocks except the last one\n",
        "        for i, block in enumerate(model.blocks[:-1]):\n",
        "            x = block(x)\n",
        "\n",
        "        # Get attention from the last block\n",
        "        # We're interested in how the cls token attends to the patches\n",
        "        x = model.blocks[-1].norm1(x)  # apply LN first (pre-norm)\n",
        "        qkv = model.blocks[-1].attn.qkv(x).reshape(B, x.shape[1], 3, model.blocks[-1].attn.num_heads, model.blocks[-1].attn.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        attn = (q @ k.transpose(-2, -1)) * (1.0 / np.sqrt(model.blocks[-1].attn.head_dim))\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "\n",
        "        return attn\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Get model predictions\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        # Get attention maps\n",
        "        attentions = get_attention_maps(images)  # shape: [B, H, N, N]\n",
        "\n",
        "        # Extract attention from the CLS token to all patches\n",
        "        # Average over all heads for visualization\n",
        "        cls_attentions = attentions[:, :, 0, 1:].mean(1)  # shape: [B, N-1]\n",
        "\n",
        "    # Reshape attention maps to match the image patches\n",
        "    patch_size = 4\n",
        "    num_patches = 8  # 32 // 4 = 8\n",
        "\n",
        "    plt.figure(figsize=(16, 4 * num_images))\n",
        "\n",
        "    for i in range(num_images):\n",
        "        # Original image - need to denormalize\n",
        "        img = images[i].cpu().permute(1, 2, 0).numpy()\n",
        "        img = img * np.array([0.2470, 0.2435, 0.2616]) + np.array([0.4914, 0.4822, 0.4465])\n",
        "        img = np.clip(img, 0, 1)\n",
        "\n",
        "        # Attention map\n",
        "        attn_map = cls_attentions[i].reshape(num_patches, num_patches).cpu().numpy()\n",
        "\n",
        "        # Upsample the attention map to match the image size\n",
        "        # Simple nearest-neighbor upsampling\n",
        "        attn_map = np.repeat(np.repeat(attn_map, patch_size, axis=0), patch_size, axis=1)\n",
        "\n",
        "        # Color indicates attention strength\n",
        "        plt.subplot(num_images, 3, i*3 + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(f\"Original: {classes[labels[i]]}\\nPredicted: {classes[predicted[i]]}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(num_images, 3, i*3 + 2)\n",
        "        plt.imshow(attn_map)\n",
        "        plt.title(\"Attention Map\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(num_images, 3, i*3 + 3)\n",
        "        plt.imshow(img)\n",
        "        plt.imshow(attn_map, alpha=0.5, cmap='jet')  # overlay with some transparency\n",
        "        plt.title(\"Overlay\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('vit_attention_maps.png')\n",
        "    plt.close()  # close to avoid display issues"
      ],
      "metadata": {
        "id": "NsQbPj6Sy4qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main execution"
      ],
      "metadata": {
        "id": "j81X9qOGtdGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Create models directory\n",
        "    os.makedirs('metrics', exist_ok=True)\n",
        "\n",
        "    # Train the model\n",
        "    # You can customize these hyperparameters\n",
        "    model, best_acc, metrics = train_vit_cifar10(\n",
        "        epochs=100,      # max epochs\n",
        "        batch_size=128,  # reduce if OOM\n",
        "        lr=1e-3,         # tried 5e-4 and 3e-3, this works best\n",
        "        warmup_epochs=5, # helps stabilize training\n",
        "        model_name='ViT' # for saving metrics\n",
        "    )\n",
        "\n",
        "    # Visualize attention if we did well\n",
        "    data_module = CIFAR10DataModule(batch_size=4)\n",
        "    data_module.setup()\n",
        "    val_loader = data_module.val_dataloader()\n",
        "\n",
        "    print(\"Visualizing attention maps\")\n",
        "    visualize_attention(model, val_loader, device)"
      ],
      "metadata": {
        "id": "LVe8Tjaoy7oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "wyWnRhwHthLF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X91a9xRLtjfb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}