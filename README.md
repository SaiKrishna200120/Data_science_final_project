# Enhancing Image Classification with Vision Transformers

**Author:** Sai Krishna Vavilli (SaiKrishna200120)

**University:** University of Hertfordshire 

**Student ID:** 23022047

---
# Work done

# All of the code in this repository is my individual work. My work, along with the derivative contributions from other authors, is clearly documented here, please refer codebase referennce .The theoretical foundation and conceptual ideas have been informed by the contributions from various research papers, which are fully referenced in the paper refernence section.

### **References**

# Codebase Reference
- [s-chh / PyTorch-Scratch-Vision-Transformer-ViT](https://github.com/s-chh/PyTorch-Scratch-Vision-Transformer-ViT)
- [Michel-Liao / scratch-vit](https://github.com/Michel-Liao/scratch-vit)
- [Taeyoung96 / Resnet-from-scratch](https://github.com/Taeyoung96/Resnet-from-scratch)
- [huggingface / pytorch-image-models](https://github.com/huggingface/pytorch-image-models)



# Paper reference
- [Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. A., Kaiser, ≈Å., & Polosukhin, I. (2017). *Attention is All You Need*](https://arxiv.org/abs/1706.03762)
- [Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). *ImageNet Classification with Deep Convolutional Neural Networks*](https://www.cs.toronto.edu/~fritz/absps/imagenet.pdf)
- [Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., & Houlsby, N. *An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale*](https://arxiv.org/abs/2010.11929)
- [Hadhoud, Y., Mekhaznia, T., Bennour, A., Amroune, M., Kurdi, N. A., Aborujilah, A. H., & Al-Sarem, M. (2024). *From Binary to Multi-Class Classification: A Two-Step Hybrid CNN-ViT Model for Chest Disease Classification Based on X-Ray Images*.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11639898/)

## Project Overview

This repository contains the code and documentation for my final dissertation project at the University of Hertfordshire. The project investigates the use of Vision Transformers (ViT) for image classification using the CIFAR-10 dataset and compares their performance to traditional Convolutional Neural Networks (CNNs) such as ResNet, as well as a hybrid model that combines elements of both approaches.

### Project Summary

Building a Vision Transformer (ViT) model and comparing its performance with other models such as ResNet and a hybrid model. The goal is to analyze how self-attention in ViTs impacts classification accuracy in comparison to traditional CNN-based approaches. The hybrid model explores the possibility of leveraging both global attention and local feature extraction.

###  Vision Transformer Architecture

![Vision Transformer Architecture](https://viso.ai/wp-content/uploads/2021/09/vision-transformer-vit.png)

*Source: [viso.ai](https://viso.ai/computer-vision/vision-transformer-vit/)*


###  ResNet Architecture

![ResNet Architecture](https://insightfultscript.com/collections/programming/neural-network/resnet/resnet-arch.webp)

*Source: [InsightfulTscript](https://insightfultscript.com/)*


---

### Research Questions

How does the self-attention mechanism in Vision Transformers (ViT) influence performance in image classification tasks?

What are the advantages and limitations of Vision Transformers compared to traditional Convolutional Neural Networks (CNNs) and hybrid models?

How do dataset size, training complexity, and computational requirements impact the performance of ViTs, CNNs, and hybrid models?

---

### Project Objectives

1. **Model Analysis:** Understand the inherent differences between Vision Transformers and CNNs.
2. **Performance Evaluation:** Benchmark ViT models on the CIFAR-10 dataset and compare their performance with ResNet and the hybrid model.
3. **Comparative Study:** Analyze each model's strengths and weaknesses in terms of accuracy, efficiency, and computational requirements.
4. **Research Insight:** Develop comprehensive insights to help determine the most effective approach for different image classification scenarios.

---



