# Enhancing Image Classification with Vision Transformers

**Author:** Sai Krishna Vavilli (SaiKrishna200120)

**University:** University of Hertfordshire 

**Student ID:** 23022047

---
# Work done

# All of the code in this repository is my individual work. My work, along with the derivative contributions from other authors, is clearly documented here, please refer codebase referennce .The theoretical foundation and conceptual ideas have been informed by the contributions from various research papers, which are fully referenced in the paper refernence section.

### **References**

# Codebase Reference
- [s-chh / PyTorch-Scratch-Vision-Transformer-ViT](https://github.com/s-chh/PyTorch-Scratch-Vision-Transformer-ViT)
- [Michel-Liao / scratch-vit](https://github.com/Michel-Liao/scratch-vit)
- [Taeyoung96 / Resnet-from-scratch](https://github.com/Taeyoung96/Resnet-from-scratch)
- [huggingface / pytorch-image-models](https://github.com/huggingface/pytorch-image-models)



# Paper reference
    Chen, X., Xie, S. and He, K., 2021. An empirical study of training self-supervised Vision Transformers. arXiv preprint arXiv:2104.02057.

    Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J. and Houlsby, N., 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.

    Krizhevsky, A., Sutskever, I. and Hinton, G.E., 2012. ImageNet classification with deep convolutional neural networks. Advances in Neural Information Processing Systems, 25, pp.1097–1105.

    Wu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L. and Zhang, L., 2021. CvT: Introducing convolutions to vision transformers. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp.22–31.

## Project Overview

This repository contains the code and documentation for my final dissertation project at the University of Hertfordshire. The project investigates the use of Vision Transformers (ViT) for image classification using the CIFAR-10 dataset and compares their performance to traditional Convolutional Neural Networks (CNNs) such as ResNet.

### Project Summary

Building a Vision Transformer (ViT) model and comparing its performance with other models such as ResNet . The goal is to analyze how self-attention in ViTs impacts classification accuracy in comparison to traditional CNN-based approaches.


---

### Research Questions

How does the self-attention mechanism in Vision Transformers (ViT) influence performance in image classification tasks?

What are the advantages and limitations of Vision Transformers compared to traditional Convolutional Neural Networks (CNNs)models?

How do dataset size, training complexity, and computational requirements impact the performance of ViTs, CNNs models?

---

### Project Objectives

1. **Model Analysis:** Understand the inherent differences between Vision Transformers and CNNs.
2. **Performance Evaluation:** Benchmark ViT models on the CIFAR-10 dataset and compare their performance with ResNet .
3. **Comparative Study:** Analyze each model's strengths and weaknesses in terms of accuracy, efficiency, and computational requirements.
4. **Research Insight:** Develop comprehensive insights to help determine the most effective approach for different image classification scenarios.

---



